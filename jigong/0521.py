# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_WouWdEVLFOaP4g2r7KN0blhZYr5K7j
"""

!pip install numpy==1.23.5 pmdarima==2.0.3 xgboost==2.0.3 tensorflow==2.12.0 --force-reinstall --quiet

# ===============================================================
# 0)  INSTALLS  (Colab 전용 — 로컬이면 주석)
# ===============================================================
!pip install -q --force-reinstall numpy==1.23.5 pmdarima==2.0.3 xgboost==2.0.3 tensorflow==2.12.0
!apt-get -qq update && apt-get -qq install -y libgdal-dev fonts-nanum
!pip install -q --no-binary :all: fiona shap

# ===============================================================
# 1)  IMPORT & CONFIG
# ===============================================================
from pathlib import Path
import numpy as np, pandas as pd, geopandas as gpd
import matplotlib.pyplot as plt, seaborn as sns, folium, shap, warnings, logging
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score
from tqdm import tqdm

warnings.filterwarnings("ignore")
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
plt.rcParams['font.family'] = 'NanumGothic'; plt.rcParams['axes.unicode_minus'] = False

DATA_DIR   = Path('/content')
OUTPUT_DIR = Path('/content/output'); OUTPUT_DIR.mkdir(exist_ok=True)
RAND_SEED  = 42; np.random.seed(RAND_SEED)

HIST_MONTHS, TEST_MONTHS = 60, 12
LAGS = [1,3,12,24]
TARGET_CATS = ['living','food','fashion','digital','etc']

CATEGORY_MAP = {
    "가구/인테리어":"living","생활/건강":"living","식품":"food",
    "패션의류":"fashion","패션잡화":"fashion","화장품/미용":"fashion",
    "디지털/가전":"digital",
    "도서/음반":"etc","스포츠/레저":"etc","출산/육아":"etc","기타":"etc"
}

# ===============================================================
# 2)  유틸리티
# ===============================================================
def read_csv_auto(fp, encodings=("utf-8-sig","cp949","euc-kr")):
    for enc in encodings:
        try:
            return pd.read_csv(fp, encoding=enc)
        except UnicodeDecodeError:
            pass
    raise UnicodeDecodeError(f"❌ 인코딩 실패: {fp}")

def wmape(y, yhat):
    return np.abs(y - yhat).sum() / np.abs(y).sum() * 100

def smape(y, yhat, eps=1e-8):
    return np.mean(2 * np.abs(yhat - y) /
                   (np.abs(y) + np.abs(yhat) + eps)) * 100

# ------------------------------------------------------------------
# 3. 정적 지표 함수 (지하철, 보도연장 등)  ─ 기존과 동일
# ------------------------------------------------------------------
def subway_station_cnt(path):
    df = read_csv_auto(path).rename(columns=lambda c: c.strip())
    df = df.rename(columns={"자치구":"gu_name"})
    return df.set_index("gu_name")["역개수"].astype(int).rename("sub_cnt")

def day_night_pop(path):
    df = pd.read_csv(path, encoding="cp949", header=[0,1])
    df = df[(df[('성별','성별')]=='계') & (df[('연령별','연령별')]=='합계')]
    df['gu_name'] = clean_gu(df[('행정구역별','행정구역별')])
    df['night_pop'] = df[('2020','상주인구')].astype(str).str.replace(",","").astype(float)
    df['day_pop']   = df[('2020','주간 인구')].astype(str).str.replace(",","").astype(float)
    return df[['gu_name','night_pop','day_pop']].set_index('gu_name')

def sidewalk_km(path):
    df = read_csv_auto(path).rename(columns=lambda c: c.strip())
    len_col = [c for c in df.columns if "연장" in c][0]
    df['gu_name'] = clean_gu(df['자치구별(2)'])
    df['m'] = df[len_col].astype(str).str.replace(r"[^0-9\.]","",regex=True).astype(float)
    return (df.groupby('gu_name')['m'].sum()/1000).rename('sidewalk_km')

def vacancy_rate(path, quarter="2024년 4분기", mapping=None):
    if mapping is None: mapping = MARKET2GU
    df = pd.read_csv(path, encoding="cp949").rename(columns=lambda c: c.strip())
    df = (df.query("분류=='서울'")
            .assign(gu_name=lambda d: d['분류.2'].map(mapping))
            .dropna(subset=['gu_name'])
            .assign(val=lambda d: pd.to_numeric(d[quarter].str.rstrip('%'), errors='coerce'))
            .groupby('gu_name')['val'].mean()
            .rename('vacancy_pct'))
    return df

def future_population(path):
    df = pd.read_csv(path, encoding="cp949")
    df.columns = df.columns.astype(str).str.strip()
    df = df.rename(columns={"자치구별(2)":"gu_name","성별(1)":"gender"})
    df = df[df['gender']=='계']
    df['gu_name'] = clean_gu(df['gu_name'])
    pop_cols = [c for c in df.columns if c.startswith("2042")]
    for c in pop_cols:
        df[c] = df[c].astype(str).str.replace(",","").astype(float)
    df['pop2042'] = df[pop_cols].sum(axis=1)
    return df.set_index('gu_name')['pop2042']

def build_static(w,
                 subway_csv, daynight_csv,
                 sidewalk_csv, vacancy_csv,
                 future_pop_csv):
    base = w[['gu_name']].drop_duplicates().set_index('gu_name')
    s_sub  = subway_station_cnt(subway_csv).to_frame()
    s_side = sidewalk_km(sidewalk_csv).to_frame()
    s_vac  = vacancy_rate(vacancy_csv).to_frame()
    df_dn  = day_night_pop(daynight_csv)
    s_pop  = future_population(future_pop_csv).to_frame()

    for df in (s_sub,s_side,s_vac,df_dn,s_pop):
        df.index.name='gu_name'

    static = base.join([s_sub,s_side,s_vac,df_dn,s_pop], how='left').reset_index()
    static[['sub_cnt','sidewalk_km']] = static[['sub_cnt','sidewalk_km']].fillna(0)
    static['vacancy_pct'] = static['vacancy_pct'].fillna(static['vacancy_pct'].mean())
    static['pop2042']     = static['pop2042'].fillna(0)

    static['access_idx'] = (
        static[['sub_cnt','sidewalk_km']]
        .apply(lambda col: (col-col.mean())/(col.std(ddof=0) or 1))
        .mean(axis=1)
    )
    return static

# ===============================================================
# 3)  Feature Engineering (휴일 더미 제거★)
# ===============================================================
def make_feature_df(series,
                    lags   = (1, 2, 3, 7, 14, 30),
                    add_ma = (3,),
                    add_pct=True):
    """시리즈 → 학습용 피처 DF (휴일 피처 없음)"""

    series = series.sort_index()
    df = pd.DataFrame({'y': np.log1p(series)})

    # 월·요일 seasonality
    df['m_sin'] = np.sin(2 * np.pi * series.index.month   / 12)
    df['m_cos'] = np.cos(2 * np.pi * series.index.month   / 12)
    df['d_sin'] = np.sin(2 * np.pi * series.index.dayofweek / 7)
    df['d_cos'] = np.cos(2 * np.pi * series.index.dayofweek / 7)

    # lag
    for l in lags:
        df[f'lag{l}'] = df['y'].shift(l)

    # 이동평균
    for w in add_ma:
        df[f'ma{w}'] = df['y'].rolling(w).mean()

    # 증감률
    if add_pct:
        df['mom_pct'] = series.pct_change().replace([np.inf, -np.inf], np.nan)

    return df.replace([np.inf, -np.inf], np.nan).dropna()

# ===============================================================
# (NEW) import 보강 ― 파일 위쪽 한곳이면 충분
# ===============================================================
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score          # 이미 한 번 import돼 있어도 OK

import xgboost as xgb
import numpy as np


# ── custom wMAPE 함수 (변경 없음) ──
def wmappeval(preds: np.ndarray, dtrain: xgb.DMatrix):
    y_true = np.expm1(dtrain.get_label())
    y_pred = np.expm1(preds)
    wmape  = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))
    return 'wMAPE', wmape

# ── RandomizedSearchCV 결과 반영된 Forecast 함수 ──
def xgb_forecast_native(series,
                        test_days       = TEST_DAYS,
                        val_ratio       = 0.2,
                        num_boost_round = 300,
                        early_stopping  = 20,
                        max_depth       = 9,       # Best: 9
                        learning_rate   = 0.05,    # Best: 0.05
                        reg_alpha       = 0.1,     # Best: 0.1
                        reg_lambda      = 0.5,     # Best: 0.5
                        subsample       = 0.9,     # Best: 0.9
                        colsample_bytree= 0.9):    # Best: 0.9

    # 1) feature·label 생성
    df_feat = make_feature_df(series)
    X, y    = df_feat.drop(columns='y').values, df_feat['y'].values

    # 2) hold-out TEST 분리
    X_all, X_te = X[:-test_days], X[-test_days:]
    y_all, y_te = y[:-test_days], y[-test_days:]

    # 3) TRAIN/VAL split (80:20)
    split_i     = int(len(X_all) * (1 - val_ratio))
    X_tr, X_val = X_all[:split_i], X_all[split_i:]
    y_tr, y_val = y_all[:split_i], y_all[split_i:]

    # 4) DMatrix 생성
    dtrain = xgb.DMatrix(X_tr,    label=y_tr)
    dval   = xgb.DMatrix(X_val,  label=y_val)
    dtest  = xgb.DMatrix(X_te,   label=y_te)

    # 5) 하이퍼파라미터 (RandomizedSearchCV 최적값)
    params = {
        'objective':        'reg:squarederror',
        'tree_method':      'hist',
        'max_depth':        max_depth,
        'learning_rate':    learning_rate,
        'subsample':        subsample,
        'colsample_bytree': colsample_bytree,
        'reg_alpha':        reg_alpha,
        'reg_lambda':       reg_lambda,
        'seed':             RAND_SEED
    }

    # 6) 학습 (wMAPE 모니터링 + 조기종료)
    bst = xgb.train(
        params,
        dtrain,
        num_boost_round       = num_boost_round,
        evals                 = [(dtrain, 'train'), (dval, 'val')],
        feval                 = wmappeval,
        maximize              = False,
        early_stopping_rounds = early_stopping,
        verbose_eval          = False
    )

    # 7) best_iteration 기준 예측
    best_it  = bst.best_iteration
    yhat_tr  = np.expm1(bst.predict(dtrain,  iteration_range=(0, best_it)))
    yhat_val = np.expm1(bst.predict(dval,    iteration_range=(0, best_it)))
    yhat_te  = np.expm1(bst.predict(dtest,   iteration_range=(0, best_it)))

    y_tr_org  = np.expm1(y_tr)
    y_val_org = np.expm1(y_val)
    y_te_org  = np.expm1(y_te)

    # 8) 지표 계산
    r2_tr   = r2_score(y_tr_org,  yhat_tr)
    r2_val  = r2_score(y_val_org, yhat_val)
    wm_tr   = wmape(y_tr_org,     yhat_tr)
    wm_val  = wmape(y_val_org,    yhat_val)

    print(f"[INFO] best_iter       : {best_it}")
    print(f"[INFO] R²    (tr / val) : {r2_tr:.3f} / {r2_val:.3f}")
    print(f"[INFO] wMAPE (tr / val) : {wm_tr:6.2f}% / {wm_val:6.2f}%")

    # 9) test 예측값 + 지표 반환
    return yhat_te, y_te_org, r2_tr, r2_val, wm_tr, wm_val

# ===============================================================
# 5)  Raw → Panel (wide) 변환
# ===============================================================
DATA_DIR = Path('/content')
files    = sorted(DATA_DIR.glob('DWC_*.csv'))
raw      = pd.concat([read_csv_auto(f) for f in files], ignore_index=True)

raw['date'] = pd.to_datetime(raw['배송년월일'], format='%Y%m%d')
start = raw['date'].max() - pd.DateOffset(months=HIST_MONTHS - 1)
raw   = raw[raw['date'] >= start]

cat_cols = [c for c in raw.columns if c.startswith('대분류')]
long = raw.melt(id_vars=['수하인_구명', 'date'],
                value_vars=cat_cols,
                var_name='cat', value_name='vol')
long['group'] = long['cat'].str.replace(r'^대분류_착지물동량\s*', '', regex=True)\
                           .map(CATEGORY_MAP)

w = (long.groupby(['date', '수하인_구명', 'group'])['vol'].sum()
          .unstack(fill_value=0).reset_index()
          .rename(columns={'수하인_구명': 'gu_name'}))

# ── 0. 상권 → 자치구 매핑 사전(필요 시 밖에서 import 해 와도 됨)
MARKET2GU = {
    # ── 도심권
    '광화문':'종로구','동대문':'종로구','북촌':'종로구','서촌':'종로구','종로':'종로구',
    '명동':'중구','남대문':'중구','시청':'중구','을지로':'중구','충무로':'중구','방산시장':'중구',
    # ── 강남권
    '강남':'강남구','강남대로':'강남구','논현역':'강남구','도산대로':'강남구','신사역':'강남구',
    '압구정':'강남구','청담':'강남구','테헤란로':'강남구',
    '교대역':'서초구','남부터미널':'서초구',
    # ── 영등포·신촌권
    '영등포역':'영등포구','당산역':'영등포구',
    '공덕역':'마포구','홍대/합정':'마포구','동교/연남':'마포구','망원역':'마포구',
    '신촌/이대':'서대문구',
    # ── 기타권
    '건대입구':'광진구','경희대':'동대문구','청량리':'동대문구','장안동':'동대문구',
    '상봉역':'중랑구','상계역':'노원구',
    '까치산역':'강서구','화곡':'강서구',
    '뚝섬':'성동구','왕십리':'성동구',
    '천호':'강동구','잠실/송파':'송파구','잠실새내역':'송파구',
    '목동':'양천구','양재역':'서초구',
    '노량진':'동작구','사당':'동작구',
    '이태원':'용산구','용산역':'용산구','숙명여대':'용산구',
    '독산/시흥':'금천구','연신내':'은평구','불광역':'은평구',
    '성신여대':'성북구','미아사거리':'강북구','수유':'강북구',
    '오류동역':'구로구','신림역':'관악구','서울대입구역':'관악구',
}

# ===============================================================
# 6)  예측 루프
# ===============================================================

results, tv_scores = [], []

for gu in w['gu_name'].unique():
    gdf = (w[w['gu_name'] == gu]
             .set_index('date')
             .sort_index())

    for cat in TARGET_CATS:
        s = gdf[cat]
        if s.sum() == 0: continue

        # 6개 언패킹
        pred, true, r2_tr, r2_val, wm_tr, wm_val = xgb_forecast(
            s, test_days=TEST_DAYS, plot_curve=False
        )

        dates = s.index[-TEST_DAYS:]
        results.extend([[gu, cat, d, a, p]
                        for d, a, p in zip(dates, true, pred)])
        tv_scores.append([gu, cat, r2_tr, r2_val, wm_tr, wm_val])


df = pd.DataFrame(results, columns=['gu','cat','date','actual','pred'])\
         .reset_index(drop=True)      # 인덱스 → RangeIndex
df.index.name = None                  # 인덱스에 구(gu) 같은 이름이 남아 있지 않게


# ── 저물량(하위 30 %·800건 미만) 제외

def drop_low(df, pct=0.00, abs_cut=0):
    df = df.reset_index(drop=True)           # ★ 인덱스 → 컬럼 강제 변환
    def _mask(g):
        th = max(g.actual.quantile(pct), abs_cut)
        return g[g.actual >= th]
    return df.groupby(['gu', 'cat'], group_keys=False).apply(_mask)



df_filt = drop_low(df)
         # 저물량 행 제거
df_filt = df_filt.reset_index(drop=True)   # ★ INDEX 정리


df_tv = pd.DataFrame(
    tv_scores,
    columns=[
      'gu','cat',
      'R2_train','R2_val',
      'wMAPE_train','wMAPE_val'
    ]
)
print("\n=== 학습/검증 R² · wMAPE ===")
display(df_tv.sort_values('wMAPE_val'))

# ─────────────────────────────────────────────────────────────
# 평균 지표 계산·출력
# ─────────────────────────────────────────────────────────────
avg_r2_tr    = df_tv['R2_train'].mean()
avg_r2_val   = df_tv['R2_val'].mean()
avg_wmape_tr = df_tv['wMAPE_train'].mean()
avg_wmape_val= df_tv['wMAPE_val'].mean()

print(f"\n=== 전체 평균 지표 ===")
print(f"R²  (train) : {avg_r2_tr:.3f}")
print(f"R²  (val)   : {avg_r2_val:.3f}")
print(f"wMAPE (train): {avg_wmape_tr:5.2f}%")
print(f"wMAPE (val)  : {avg_wmape_val:5.2f}%")

from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import make_scorer
import xgboost as xgb
import numpy as np

# 1) wMAPE 스코어러 정의 (기존과 동일)
def wmape_sklearn(y_true, y_pred):
    y_true = np.expm1(y_true)
    y_pred = np.expm1(y_pred)
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

wmape_scorer = make_scorer(wmape_sklearn, greater_is_better=False)

# 2) 피처·라벨 준비 (기존과 동일)
df_feat = make_feature_df(series)
X = df_feat.drop(columns='y').values
y = df_feat['y'].values

# 3) 시계열 4-fold
tscv = TimeSeriesSplit(n_splits=4, test_size=TEST_DAYS)

# 4) 분포(그리드) 정의
param_dist = {
    'max_depth':        [5, 7, 9, 11],
    'learning_rate':    [0.005, 0.01, 0.02, 0.05],
    'reg_alpha':        [0.0, 0.1, 0.5],
    'reg_lambda':       [0.0, 0.5, 1.0],
    'subsample':        [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# 5) RandomizedSearchCV 세팅
rscv = RandomizedSearchCV(
    estimator = xgb.XGBRegressor(
        objective='reg:squarederror',
        tree_method='hist',
        n_estimators=150,
        seed=RAND_SEED,
        verbosity=0
    ),
    param_distributions = param_dist,
    n_iter   = 50,           # 시도할 조합 수 (예: 50)
    scoring  = wmape_scorer,
    cv       = tscv,
    random_state = RAND_SEED,
    verbose  = 2,
    n_jobs   = -1
)

# 6) 탐색 실행
rscv.fit(X, y)

# 7) 결과 확인
best = rscv.best_params_
print("▶ Best Params   :", best)
print("▶ Best wMAPE avg:", -rscv.best_score_ * 100, "%")

# 한글 폰트 설치
!apt-get -qq update
!apt-get -qq install -y fonts-nanum

import logging, warnings, matplotlib.pyplot as plt

# 1) findfont 로그 레벨 ERROR로 낮추기
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# 2) 한글 글리프 누락 경고 무시
warnings.filterwarnings(
    "ignore",
    message=".*missing from font.*",
    module="matplotlib.font_manager"
)

# 3) 나눔고딕 폰트 적용 (Colab 전용) 및 마이너스 기호 깨짐 방지
plt.rcParams['font.family']        = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False

# --------------------------------------------------
# 9) PREDICT NEXT MONTH & SCORE (XGBoost - feature_names 강제 동기화)
# --------------------------------------------------
print('[7] Predict & Score')

# ① 최신 월(가장 최근 관측치)만 추출 → 예측 대상 데이터
latest = panel.groupby('gu_name').tail(1)

# ② 실제 학습에 사용된 feature 이름 추출
if best_model is not None:
    model_feat_names = list(best_model.estimators_[0].feature_names_in_)
else:
    model_feat_names = list(next(iter(model_dict.values())).feature_names_in_)

# ③ panel에 모델이 요구하는 feature가 없으면 NaN(또는 0)으로 강제 추가
panel_fixed = panel.copy()
for f in model_feat_names:
    if f not in panel_fixed.columns:
        panel_fixed[f] = 0  # 또는 np.nan도 가능 (예측엔 0이 더 안전)

# ④ panel의 feature 컬럼 순서를 모델 순서에 맞게 정렬
X_df = panel_fixed[model_feat_names]
X_future = X_df.loc[latest.index]

# ⑤ 예측
if best_model is not None:
    yhat = best_model.predict(X_future)
else:
    yhat = np.column_stack([
        model_dict[c].predict(X_future)
        for c in TARGET_CATS
    ])

# ⑥ 예측 결과 DataFrame화
n_targets = yhat.shape[1] if len(yhat.shape) > 1 else 1
target_cats_used = TARGET_CATS[:n_targets]
ypred = pd.DataFrame(
    yhat,
    columns=[f'pred_{c}' for c in target_cats_used],
    index=latest.index
)

# ⑦ 자치구 이름과 예측값 합치기
res = latest[['gu_name']].join(ypred)

# ⑧ 수요 지표(demand_idx) 계산
res['demand_idx'] = res.filter(like='pred_').max(axis=1)
res['demand_idx'] /= res['demand_idx'].max() if res['demand_idx'].max() != 0 else 1

# ⑨ 추천 카테고리(max_cat)
res['max_cat'] = (
    res.filter(like='pred_')
       .idxmax(axis=1)
       .str.replace('pred_', '')
)

# ⑩ static과 병합
res = res.merge(static, on='gu_name', how='left')

# ⑪ vacancy_idx 계산
res['vacancy_idx'] = 1 - res['vacancy_pct'] / res['vacancy_pct'].max() if res['vacancy_pct'].max() != 0 else 1

# ⑫ 최종 score 계산
res['score'] = (
    res['demand_idx'] * 0.6 +
    res['vacancy_idx'] * 0.3 +
    res['access_idx']  * 0.1
)

# ⑬ 자치구별 예측값 출력
ypred_with_gu = latest[['gu_name']].join(ypred)
print(ypred_with_gu.to_string())

# =============================================================================
# 시각화 블록 (XGB MultiOutputRegressor 버전, 재학습 없이 실행)
# =============================================================================
import warnings, logging, matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import numpy as np, pandas as pd
from xgboost import XGBRegressor

# ───────────────────────────────────────── 환경 설정 (동일) ─────────────────────────────────────────
font_path = '/content/NanumGothic.ttf'
fm.fontManager.addfont(font_path)
plt.rcParams['font.family']        = fm.FontProperties(fname=font_path).get_name()
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings("ignore", category=UserWarning,
                        message=".*missing from font\\(s\\).*")
warnings.filterwarnings("ignore", category=FutureWarning,
                        message=".*Passing `palette` without assigning `hue`.*")
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# =============================================================================
# 1) 검증 세트 예측값 (y_pred_final) 계산
# =============================================================================
y_pred_final = pd.DataFrame(
    best_model.predict(Xte),
    columns=Yte.columns,
    index=Yte.index
)

# =============================================================================
# 2) PCA 설명 분산 비율 시각화
#    — pca.explained_variance_ratio_ 사용
# =============================================================================
explained = pca.explained_variance_ratio_ * 100

plt.figure(figsize=(8, 4))
sns.barplot(
    x=['PC1', 'PC2', 'PC3'],
    y=explained[:3],
    color='skyblue'
)
for idx, val in enumerate(explained[:3]):
    plt.text(
        idx,
        val + 1,
        f'{val:.1f}%',
        ha='center',
        va='bottom',
        fontsize=10
    )
plt.title('PCA 주성분별 설명 분산 비율 (%)', fontsize=14)
plt.xlabel('주성분', fontsize=12)
plt.ylabel('설명 분산 비율 (%)', fontsize=12)
plt.ylim(0, 100)
plt.tight_layout()
plt.show()


# =============================================================================
# 3) 자치구별 정규화 지표 비교
#    — res DataFrame에 ['pop_idx', 'access_idx', 'vacancy_idx'] 컬럼이 있다고 가정
# =============================================================================
norm_vars = ['pop_idx', 'access_idx', 'vacancy_idx']
existing = [v for v in norm_vars if v in res.columns]

if existing:
    melted = res[['gu_name'] + existing].melt(
        id_vars='gu_name',
        var_name='지표',
        value_name='값'
    )

    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=melted,
        x='gu_name',
        y='값',
        hue='지표',
        palette='Set2'
    )
    plt.title('자치구별 정규화 지표 비교', fontsize=14)
    plt.xlabel('자치구', fontsize=12)
    plt.ylabel('정규화 값', fontsize=12)
    plt.xticks(rotation=45)
    plt.legend(title='지표', loc='best', frameon=True, fontsize=10, title_fontsize=11)
    plt.tight_layout()
    plt.show()


# =============================================================================
# 4) 자치구별 예측 물동량 총합 (pred_total) 시각화
#    — res DataFrame에 pred_… 컬럼이 이미 계산되어 있다고 가정
# =============================================================================
pred_cols = [c for c in res.columns if isinstance(c, str) and c.startswith('pred_')]
res['pred_total'] = res[pred_cols].sum(axis=1)

plt.figure(figsize=(12, 6))
sns.barplot(
    data=res.sort_values('pred_total', ascending=False),
    x='gu_name',
    y='pred_total',
    color='coral'
)
plt.title('자치구별 예측 물동량 총합', fontsize=14)
plt.xlabel('자치구', fontsize=12)
plt.ylabel('예측 총합', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# =============================================================================
# 5) 자치구별 최종 종합 점수 (score) 시각화
#    — res DataFrame에 score 컬럼이 이미 계산되어 있다고 가정
# =============================================================================
plt.figure(figsize=(12, 6))
sns.barplot(
    data=res.sort_values('score', ascending=False),
    x='gu_name',
    y='score',
    color='steelblue'
)
plt.title('자치구별 최종 종합 점수', fontsize=14)
plt.xlabel('자치구', fontsize=12)
plt.ylabel('점수', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# =============================================================================
# 6) XGBoost Feature Importances  (RandomForest → XGBRegressor 로 변경)
# =============================================================================
# MultiOutputRegressor 내부 각 XGBRegressor 에서 gain 기반 중요도를 집계
fi_dicts = []
feature_names = Xtr.columns.tolist()

for est in best_model.estimators_:                     # 각 타깃별 XGBRegressor
    booster = est.get_booster()
    # gain 기반 중요도 dict {f0:…, f1:…}
    scores = booster.get_score(importance_type='gain')
    # get_score 는 feature 순서를 f0,f1,… 로 반환 → 인덱스 맵 필요
    imp = np.zeros(len(feature_names))
    for f, val in scores.items():
        if f in feature_names:
            idx = feature_names.index(f)
            imp[idx] = val
    fi_dicts.append(imp)


mean_importances = np.mean(fi_dicts, axis=0)
df_fi = (pd.DataFrame({'feature': feature_names,
                       'importance': mean_importances})
           .sort_values('importance', ascending=False))

top_n = 10
plt.figure(figsize=(8, 6))
sns.barplot(
    data=df_fi.head(top_n),
    x='importance',
    y='feature',
    palette='viridis'
)
plt.title('XGBoost Feature Importances (상위 10개)', fontsize=14)
plt.xlabel('중요도 (Gain 기반)', fontsize=12)
plt.ylabel('피처', fontsize=12)
plt.tight_layout()
plt.show()


# =============================================================================
# 7) 검증 세트 실제 vs 예측 산점도
#    — Xte, Yte, y_pred_final 사용
# =============================================================================
categories = Yte.columns.tolist()
plt.figure(figsize=(16, 4 * len(categories)))

for i, cat in enumerate(categories, start=1):
    plt.subplot(len(categories), 1, i)
    plt.scatter(
        Yte[cat],
        y_pred_final[cat],
        alpha=0.6,
        color='teal',
        edgecolor='k',
        s=40
    )
    max_val = max(Yte[cat].max(), y_pred_final[cat].max())
    plt.plot([0, max_val], [0, max_val], 'r--', linewidth=1)
    plt.title(f'{cat} 실제 vs 예측 (검증 세트)', fontsize=12)
    plt.xlabel('실제 값', fontsize=10)
    plt.ylabel('예측 값', fontsize=10)
    plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()


# =============================================================================
# 8) 검증 세트 잔차 분포 (Residual Distribution, KDE)
#    — y_pred_final, Yte 사용
# =============================================================================
plt.figure(figsize=(8, 6))
for cat in categories:
    residuals = (y_pred_final[cat] - Yte[cat]).dropna()
    sns.kdeplot(
        residuals,
        fill=True,
        alpha=0.4,
        label=cat
    )

plt.title('검증 세트 잔차 분포 (Residual Distribution)', fontsize=14)
plt.xlabel('잔차 (예측 - 실제)', fontsize=12)
plt.ylabel('밀도', fontsize=12)
plt.legend(title='카테고리', fontsize=10, title_fontsize=11)
plt.tight_layout()
plt.show()

# ------------------------------------------------------------
# 11) CATEGORY-SPECIFIC MAPS  ⚑ auto-detect gu_name column
# ------------------------------------------------------------
import geopandas as gpd, pandas as pd, folium, numpy as np
from branca.colormap import linear
from folium.features   import GeoJson, GeoJsonTooltip
from pathlib import Path, PurePath
import re, warnings

def _normalize_gu(s: pd.Series) -> pd.Series:
    """'서울특별시 종로구' → '종로구' 로, 공백·접두사 제거"""
    return (
        s.astype(str)
         .str.replace(r'\s+', ' ', regex=True).str.strip()
         .str.replace(r'^서울특별시\s*', '', regex=True)
         .str.replace(r'^서울\s*',       '', regex=True)
         .str.replace(r'(특별|광역)?시$', '', regex=True)   # …시 접미사 제거
    )

def _find_gu_column(gdf: gpd.GeoDataFrame) -> str|None:
    """25개 이상 ‘…구’(또는 군·시) 로 끝나는 값이 있는 첫 번째 컬럼 반환"""
    for c in gdf.columns:
        if gdf[c].dtype == object:
            vals = _normalize_gu(gdf[c])
            uniq = vals.unique()
            # ‘…구’ ‧ ‘…군’ ‧ ‘…시’ 로 끝나는 값 개수
            gu_like = [v for v in uniq if re.search(r'(구|군|시)$', str(v))]
            if len(gu_like) >= 25:
                return c
    return None

def make_maps(
        res: pd.DataFrame,             # 예측 결과 (25×N)
        shp_path: str|Path,            # 자치구 경계 shp
        out_dir : str|Path             # HTML 저장 폴더
    ) -> None:

    out_dir = Path(out_dir); out_dir.mkdir(exist_ok=True, parents=True)

    # 1) 경계 로드 & WGS84
    g = (gpd.read_file(shp_path, encoding='cp949', errors='ignore')
            .to_crs(epsg=4326))

    # 2) gu_name 컬럼 찾기
    col = _find_gu_co_

def clean_gu(s: pd.Series) -> pd.Series:
    """
    1) 전각 공백( \u3000 ) 제거
    2) 탭·개행 ·일반 공백 strip
    3) '서울특별시 ' 같은 접두어 제거(있다면)
    """
    return (s.astype(str)
              .str.replace('\u3000', '', regex=False)      # 전각 공백
              .str.strip()                                 # 양쪽 공백·탭
              .str.replace(r'^서울특별시\s*', '', regex=True))  # 접두어

def load_gu_polygons_fixed(shp_path: str) -> gpd.GeoDataFrame:
    g = (gpd.read_file(shp_path, encoding='cp949')
           .to_crs(epsg=4326))

    # LABEL → '지역생활권(강남구_대치도곡)' ──> '강남구'
    g['gu_name'] = (
        g['LABEL']
        .str.extract(r'_(.+?)\)$', expand=False)   # '강남구'
        .pipe(clean_gu)
    )

    # NaN(추출 실패) 행 제거
    g = g.dropna(subset=['gu_name'])

    # ★ 중복되는 gu_name이 여전히 있는지 확인
    dup_check = g['gu_name'].value_counts().head()
    print('🔍  추출 후 gu_name 빈도 Top5\n', dup_check, '\n')

    # dissolve
    g25 = (g[['gu_name', 'geometry']]
           .dissolve(by='gu_name', as_index=False))

    assert len(g25) == 25, f'❌ 여전히 25개가 아님: {len(g25)}'
    return g25

# ──────────────────────────────────────────────────────────────
# CATEGORY-SPECIFIC MAPS  ―  한 번에 실행 가능한 완성 블록
# ──────────────────────────────────────────────────────────────
import re
from pathlib import Path

import geopandas as gpd
import pandas as pd
import folium
from branca.colormap import linear

# --------------------------------------------------------------
# 1) SHP → 25개 ‘구’ 폴리곤으로 정리
# --------------------------------------------------------------
def load_seoul_gu(shp_path: str) -> gpd.GeoDataFrame:
    # ① ‘원본 CRS’ 명시 ― KGD2002 Central Belt (ESRI:102082 ≈ EPSG:5174)
    g = (gpd.read_file(shp_path, encoding="cp949")
           .set_crs("EPSG:5174", allow_override=True)   # ★ 중요
           .to_crs(epsg=4326))

    # LABEL 속 괄호 안 첫 블록(‘강남구_…’) → '강남구'
    def _pick_gu(lbl):
        m = re.search(r"\(([^)]+)\)", str(lbl))
        if m:
            part = m.group(1).split("_")[0]
            return part if part.endswith("구") else None
        return None

    g["gu_name"] = g["LABEL"].apply(_pick_gu)
    g = g.dropna(subset=["gu_name"])

    # 같은 gu_name 끼리 dissolve 해서 25개 면 확보
    g25 = (g[["gu_name", "geometry"]]
           .dissolve(by="gu_name", as_index=False))

    assert len(g25) == 25, f"❌ 구 개수가 {len(g25)} (≠25)"
    return g25


# --------------------------------------------------------------
# 2) 카테고리별 Choropleth HTML 저장
# --------------------------------------------------------------
def save_category_maps(res_df: pd.DataFrame,
                       shp_path: str,
                       out_dir: str,
                       target_cats: list[str]) -> None:

    out_dir = Path(out_dir)
    out_dir.mkdir(exist_ok=True, parents=True)

    # (1) 경계 + 예측값 병합
    shp_gu  = load_seoul_gu(shp_path)
    res_g   = shp_gu.merge(res_df.reset_index(), on="gu_name", how="left")

    # (2) 카테고리별 지도 생성
    for cat in target_cats:
        col   = f"pred_{cat}"
        vals  = res_g[col].dropna()
        vmin, vmax = (0, 1) if vals.empty else (vals.min(), vals.max())
        if vmin == vmax:            # 값이 한 점뿐일 때
            vmax = vmin + 1e-6

        m = folium.Map(location=[37.56, 126.97],
                       zoom_start=11,
                       tiles="CartoDB positron")

        # 2-1) 색상 레이어
        folium.Choropleth(
            geo_data   = res_g.to_json(),
            data       = res_g,
            columns    = ["gu_name", col],
            key_on     = "feature.properties.gu_name",
            fill_color = "YlOrRd",
            fill_opacity = 0.8,
            line_opacity = 0.3,
            nan_fill_color = "#ffffff",
            legend_name   = f"{cat} demand"
        ).add_to(m)

        # 2-2) 툴팁(구 이름 & 예측값)
        folium.GeoJson(
            res_g,
            name="tooltip-layer",
            style_function = lambda _ : {"color":"transparent", "fillOpacity":0},
            tooltip = folium.features.GeoJsonTooltip(
                fields   = ["gu_name", col],
                aliases  = ["구", "예측"],
                localize = True,
                sticky   = False
            )
        ).add_to(m)

        # (3) 저장
        out_path = out_dir / f"map_{cat}.html"
        m.save(out_path)
        print(f"✔  saved  →  {out_path}")


# --------------------------------------------------------------
# 3) 실행 예시  (이미 res, TARGET_CATS, OUTPUT_DIR 존재)
# --------------------------------------------------------------
# 1. columns str 변환
if isinstance(res.columns, pd.MultiIndex):
    res.columns = ['_'.join(map(str, col)).strip('_') for col in res.columns]
else:
    res.columns = [str(col) for col in res.columns]

# 2. index str 변환
if isinstance(res.index, pd.MultiIndex):
    res.index = ['_'.join(map(str, idx)).strip('_') for idx in res.index]
else:
    res.index = [str(idx) for idx in res.index]

# 3. 모든 셀 내부에 tuple/dict가 남아있는지 확인 후 str로 변환
for col in res.columns:
    res[col] = res[col].apply(lambda x: str(x) if isinstance(x, (tuple, dict)) else x)

# 4. save_category_maps 실행
save_category_maps(
    res_df   = res,
    shp_path = "/content/seoul_gu.shp",
    out_dir  = OUTPUT_DIR,
    target_cats = TARGET_CATS
)




# 다시 GeoDataFrame으로 읽어서 사용!
g = gpd.read_file("/content/seoul_gu.shp", encoding="cp949")
g_ll = g.to_crs(epsg=4326)
print(g_ll.total_bounds)