# -*- coding: utf-8 -*-
"""Untitled3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v_WouWdEVLFOaP4g2r7KN0blhZYr5K7j
"""

!pip install numpy==1.23.5 pmdarima==2.0.3 xgboost==2.0.3 tensorflow==2.12.0 --force-reinstall --quiet

# ===============================================================
# 0)  INSTALLS  (Colab ì „ìš©â€Šâ€”â€Šë¡œì»¬ì´ë©´ ì£¼ì„)
# ===============================================================
!pip install -q --force-reinstall numpy==1.23.5 pmdarima==2.0.3 xgboost==2.0.3 tensorflow==2.12.0
!apt-get -qq update && apt-get -qq install -y libgdal-dev fonts-nanum
!pip install -q --no-binary :all: fiona shap

# ===============================================================
# 1)  IMPORT & CONFIG
# ===============================================================
from pathlib import Path
import numpy as np, pandas as pd, geopandas as gpd
import matplotlib.pyplot as plt, seaborn as sns, folium, shap, warnings, logging
import xgboost as xgb
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score
from tqdm import tqdm

warnings.filterwarnings("ignore")
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)
plt.rcParams['font.family'] = 'NanumGothic'; plt.rcParams['axes.unicode_minus'] = False

DATA_DIR   = Path('/content')
OUTPUT_DIR = Path('/content/output'); OUTPUT_DIR.mkdir(exist_ok=True)
RAND_SEED  = 42; np.random.seed(RAND_SEED)

HIST_MONTHS, TEST_MONTHS = 60, 12
LAGS = [1,3,12,24]
TARGET_CATS = ['living','food','fashion','digital','etc']

CATEGORY_MAP = {
    "ê°€êµ¬/ì¸í…Œë¦¬ì–´":"living","ìƒí™œ/ê±´ê°•":"living","ì‹í’ˆ":"food",
    "íŒ¨ì…˜ì˜ë¥˜":"fashion","íŒ¨ì…˜ì¡í™”":"fashion","í™”ì¥í’ˆ/ë¯¸ìš©":"fashion",
    "ë””ì§€í„¸/ê°€ì „":"digital",
    "ë„ì„œ/ìŒë°˜":"etc","ìŠ¤í¬ì¸ /ë ˆì €":"etc","ì¶œì‚°/ìœ¡ì•„":"etc","ê¸°íƒ€":"etc"
}

# ===============================================================
# 2)  ìœ í‹¸ë¦¬í‹°
# ===============================================================
def read_csv_auto(fp, encodings=("utf-8-sig","cp949","euc-kr")):
    for enc in encodings:
        try:
            return pd.read_csv(fp, encoding=enc)
        except UnicodeDecodeError:
            pass
    raise UnicodeDecodeError(f"âŒ ì¸ì½”ë”© ì‹¤íŒ¨: {fp}")

def wmape(y, yhat):
    return np.abs(y - yhat).sum() / np.abs(y).sum() * 100

def smape(y, yhat, eps=1e-8):
    return np.mean(2 * np.abs(yhat - y) /
                   (np.abs(y) + np.abs(yhat) + eps)) * 100

# ------------------------------------------------------------------
# 3. ì •ì  ì§€í‘œ í•¨ìˆ˜ (ì§€í•˜ì² , ë³´ë„ì—°ì¥ ë“±)  â”€ ê¸°ì¡´ê³¼ ë™ì¼
# ------------------------------------------------------------------
def subway_station_cnt(path):
    df = read_csv_auto(path).rename(columns=lambda c: c.strip())
    df = df.rename(columns={"ìì¹˜êµ¬":"gu_name"})
    return df.set_index("gu_name")["ì—­ê°œìˆ˜"].astype(int).rename("sub_cnt")

def day_night_pop(path):
    df = pd.read_csv(path, encoding="cp949", header=[0,1])
    df = df[(df[('ì„±ë³„','ì„±ë³„')]=='ê³„') & (df[('ì—°ë ¹ë³„','ì—°ë ¹ë³„')]=='í•©ê³„')]
    df['gu_name'] = clean_gu(df[('í–‰ì •êµ¬ì—­ë³„','í–‰ì •êµ¬ì—­ë³„')])
    df['night_pop'] = df[('2020','ìƒì£¼ì¸êµ¬')].astype(str).str.replace(",","").astype(float)
    df['day_pop']   = df[('2020','ì£¼ê°„ ì¸êµ¬')].astype(str).str.replace(",","").astype(float)
    return df[['gu_name','night_pop','day_pop']].set_index('gu_name')

def sidewalk_km(path):
    df = read_csv_auto(path).rename(columns=lambda c: c.strip())
    len_col = [c for c in df.columns if "ì—°ì¥" in c][0]
    df['gu_name'] = clean_gu(df['ìì¹˜êµ¬ë³„(2)'])
    df['m'] = df[len_col].astype(str).str.replace(r"[^0-9\.]","",regex=True).astype(float)
    return (df.groupby('gu_name')['m'].sum()/1000).rename('sidewalk_km')

def vacancy_rate(path, quarter="2024ë…„ 4ë¶„ê¸°", mapping=None):
    if mapping is None: mapping = MARKET2GU
    df = pd.read_csv(path, encoding="cp949").rename(columns=lambda c: c.strip())
    df = (df.query("ë¶„ë¥˜=='ì„œìš¸'")
            .assign(gu_name=lambda d: d['ë¶„ë¥˜.2'].map(mapping))
            .dropna(subset=['gu_name'])
            .assign(val=lambda d: pd.to_numeric(d[quarter].str.rstrip('%'), errors='coerce'))
            .groupby('gu_name')['val'].mean()
            .rename('vacancy_pct'))
    return df

def future_population(path):
    df = pd.read_csv(path, encoding="cp949")
    df.columns = df.columns.astype(str).str.strip()
    df = df.rename(columns={"ìì¹˜êµ¬ë³„(2)":"gu_name","ì„±ë³„(1)":"gender"})
    df = df[df['gender']=='ê³„']
    df['gu_name'] = clean_gu(df['gu_name'])
    pop_cols = [c for c in df.columns if c.startswith("2042")]
    for c in pop_cols:
        df[c] = df[c].astype(str).str.replace(",","").astype(float)
    df['pop2042'] = df[pop_cols].sum(axis=1)
    return df.set_index('gu_name')['pop2042']

def build_static(w,
                 subway_csv, daynight_csv,
                 sidewalk_csv, vacancy_csv,
                 future_pop_csv):
    base = w[['gu_name']].drop_duplicates().set_index('gu_name')
    s_sub  = subway_station_cnt(subway_csv).to_frame()
    s_side = sidewalk_km(sidewalk_csv).to_frame()
    s_vac  = vacancy_rate(vacancy_csv).to_frame()
    df_dn  = day_night_pop(daynight_csv)
    s_pop  = future_population(future_pop_csv).to_frame()

    for df in (s_sub,s_side,s_vac,df_dn,s_pop):
        df.index.name='gu_name'

    static = base.join([s_sub,s_side,s_vac,df_dn,s_pop], how='left').reset_index()
    static[['sub_cnt','sidewalk_km']] = static[['sub_cnt','sidewalk_km']].fillna(0)
    static['vacancy_pct'] = static['vacancy_pct'].fillna(static['vacancy_pct'].mean())
    static['pop2042']     = static['pop2042'].fillna(0)

    static['access_idx'] = (
        static[['sub_cnt','sidewalk_km']]
        .apply(lambda col: (col-col.mean())/(col.std(ddof=0) or 1))
        .mean(axis=1)
    )
    return static

# ===============================================================
# 3)  Feature Engineering (íœ´ì¼ ë”ë¯¸ ì œê±°â˜…)
# ===============================================================
def make_feature_df(series,
                    lags   = (1, 2, 3, 7, 14, 30),
                    add_ma = (3,),
                    add_pct=True):
    """ì‹œë¦¬ì¦ˆ â†’ í•™ìŠµìš© í”¼ì²˜ DF (íœ´ì¼ í”¼ì²˜ ì—†ìŒ)"""

    series = series.sort_index()
    df = pd.DataFrame({'y': np.log1p(series)})

    # ì›”Â·ìš”ì¼ seasonality
    df['m_sin'] = np.sin(2 * np.pi * series.index.month   / 12)
    df['m_cos'] = np.cos(2 * np.pi * series.index.month   / 12)
    df['d_sin'] = np.sin(2 * np.pi * series.index.dayofweek / 7)
    df['d_cos'] = np.cos(2 * np.pi * series.index.dayofweek / 7)

    # lag
    for l in lags:
        df[f'lag{l}'] = df['y'].shift(l)

    # ì´ë™í‰ê· 
    for w in add_ma:
        df[f'ma{w}'] = df['y'].rolling(w).mean()

    # ì¦ê°ë¥ 
    if add_pct:
        df['mom_pct'] = series.pct_change().replace([np.inf, -np.inf], np.nan)

    return df.replace([np.inf, -np.inf], np.nan).dropna()

# ===============================================================
# (NEW) import ë³´ê°• â€• íŒŒì¼ ìœ„ìª½ í•œê³³ì´ë©´ ì¶©ë¶„
# ===============================================================
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score          # ì´ë¯¸ í•œ ë²ˆ importë¼ ìˆì–´ë„ OK

import xgboost as xgb
import numpy as np


# â”€â”€ custom wMAPE í•¨ìˆ˜ (ë³€ê²½ ì—†ìŒ) â”€â”€
def wmappeval(preds: np.ndarray, dtrain: xgb.DMatrix):
    y_true = np.expm1(dtrain.get_label())
    y_pred = np.expm1(preds)
    wmape  = np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))
    return 'wMAPE', wmape

# â”€â”€ RandomizedSearchCV ê²°ê³¼ ë°˜ì˜ëœ Forecast í•¨ìˆ˜ â”€â”€
def xgb_forecast_native(series,
                        test_days       = TEST_DAYS,
                        val_ratio       = 0.2,
                        num_boost_round = 300,
                        early_stopping  = 20,
                        max_depth       = 9,       # Best: 9
                        learning_rate   = 0.05,    # Best: 0.05
                        reg_alpha       = 0.1,     # Best: 0.1
                        reg_lambda      = 0.5,     # Best: 0.5
                        subsample       = 0.9,     # Best: 0.9
                        colsample_bytree= 0.9):    # Best: 0.9

    # 1) featureÂ·label ìƒì„±
    df_feat = make_feature_df(series)
    X, y    = df_feat.drop(columns='y').values, df_feat['y'].values

    # 2) hold-out TEST ë¶„ë¦¬
    X_all, X_te = X[:-test_days], X[-test_days:]
    y_all, y_te = y[:-test_days], y[-test_days:]

    # 3) TRAIN/VAL split (80:20)
    split_i     = int(len(X_all) * (1 - val_ratio))
    X_tr, X_val = X_all[:split_i], X_all[split_i:]
    y_tr, y_val = y_all[:split_i], y_all[split_i:]

    # 4) DMatrix ìƒì„±
    dtrain = xgb.DMatrix(X_tr,    label=y_tr)
    dval   = xgb.DMatrix(X_val,  label=y_val)
    dtest  = xgb.DMatrix(X_te,   label=y_te)

    # 5) í•˜ì´í¼íŒŒë¼ë¯¸í„° (RandomizedSearchCV ìµœì ê°’)
    params = {
        'objective':        'reg:squarederror',
        'tree_method':      'hist',
        'max_depth':        max_depth,
        'learning_rate':    learning_rate,
        'subsample':        subsample,
        'colsample_bytree': colsample_bytree,
        'reg_alpha':        reg_alpha,
        'reg_lambda':       reg_lambda,
        'seed':             RAND_SEED
    }

    # 6) í•™ìŠµ (wMAPE ëª¨ë‹ˆí„°ë§ + ì¡°ê¸°ì¢…ë£Œ)
    bst = xgb.train(
        params,
        dtrain,
        num_boost_round       = num_boost_round,
        evals                 = [(dtrain, 'train'), (dval, 'val')],
        feval                 = wmappeval,
        maximize              = False,
        early_stopping_rounds = early_stopping,
        verbose_eval          = False
    )

    # 7) best_iteration ê¸°ì¤€ ì˜ˆì¸¡
    best_it  = bst.best_iteration
    yhat_tr  = np.expm1(bst.predict(dtrain,  iteration_range=(0, best_it)))
    yhat_val = np.expm1(bst.predict(dval,    iteration_range=(0, best_it)))
    yhat_te  = np.expm1(bst.predict(dtest,   iteration_range=(0, best_it)))

    y_tr_org  = np.expm1(y_tr)
    y_val_org = np.expm1(y_val)
    y_te_org  = np.expm1(y_te)

    # 8) ì§€í‘œ ê³„ì‚°
    r2_tr   = r2_score(y_tr_org,  yhat_tr)
    r2_val  = r2_score(y_val_org, yhat_val)
    wm_tr   = wmape(y_tr_org,     yhat_tr)
    wm_val  = wmape(y_val_org,    yhat_val)

    print(f"[INFO] best_iter       : {best_it}")
    print(f"[INFO] RÂ²    (tr / val) : {r2_tr:.3f} / {r2_val:.3f}")
    print(f"[INFO] wMAPE (tr / val) : {wm_tr:6.2f}% / {wm_val:6.2f}%")

    # 9) test ì˜ˆì¸¡ê°’ + ì§€í‘œ ë°˜í™˜
    return yhat_te, y_te_org, r2_tr, r2_val, wm_tr, wm_val

# ===============================================================
# 5)  Raw â†’ Panel (wide) ë³€í™˜
# ===============================================================
DATA_DIR = Path('/content')
files    = sorted(DATA_DIR.glob('DWC_*.csv'))
raw      = pd.concat([read_csv_auto(f) for f in files], ignore_index=True)

raw['date'] = pd.to_datetime(raw['ë°°ì†¡ë…„ì›”ì¼'], format='%Y%m%d')
start = raw['date'].max() - pd.DateOffset(months=HIST_MONTHS - 1)
raw   = raw[raw['date'] >= start]

cat_cols = [c for c in raw.columns if c.startswith('ëŒ€ë¶„ë¥˜')]
long = raw.melt(id_vars=['ìˆ˜í•˜ì¸_êµ¬ëª…', 'date'],
                value_vars=cat_cols,
                var_name='cat', value_name='vol')
long['group'] = long['cat'].str.replace(r'^ëŒ€ë¶„ë¥˜_ì°©ì§€ë¬¼ë™ëŸ‰\s*', '', regex=True)\
                           .map(CATEGORY_MAP)

w = (long.groupby(['date', 'ìˆ˜í•˜ì¸_êµ¬ëª…', 'group'])['vol'].sum()
          .unstack(fill_value=0).reset_index()
          .rename(columns={'ìˆ˜í•˜ì¸_êµ¬ëª…': 'gu_name'}))

# â”€â”€ 0. ìƒê¶Œ â†’ ìì¹˜êµ¬ ë§¤í•‘ ì‚¬ì „(í•„ìš” ì‹œ ë°–ì—ì„œ import í•´ ì™€ë„ ë¨)
MARKET2GU = {
    # â”€â”€ ë„ì‹¬ê¶Œ
    'ê´‘í™”ë¬¸':'ì¢…ë¡œêµ¬','ë™ëŒ€ë¬¸':'ì¢…ë¡œêµ¬','ë¶ì´Œ':'ì¢…ë¡œêµ¬','ì„œì´Œ':'ì¢…ë¡œêµ¬','ì¢…ë¡œ':'ì¢…ë¡œêµ¬',
    'ëª…ë™':'ì¤‘êµ¬','ë‚¨ëŒ€ë¬¸':'ì¤‘êµ¬','ì‹œì²­':'ì¤‘êµ¬','ì„ì§€ë¡œ':'ì¤‘êµ¬','ì¶©ë¬´ë¡œ':'ì¤‘êµ¬','ë°©ì‚°ì‹œì¥':'ì¤‘êµ¬',
    # â”€â”€ ê°•ë‚¨ê¶Œ
    'ê°•ë‚¨':'ê°•ë‚¨êµ¬','ê°•ë‚¨ëŒ€ë¡œ':'ê°•ë‚¨êµ¬','ë…¼í˜„ì—­':'ê°•ë‚¨êµ¬','ë„ì‚°ëŒ€ë¡œ':'ê°•ë‚¨êµ¬','ì‹ ì‚¬ì—­':'ê°•ë‚¨êµ¬',
    'ì••êµ¬ì •':'ê°•ë‚¨êµ¬','ì²­ë‹´':'ê°•ë‚¨êµ¬','í…Œí—¤ë€ë¡œ':'ê°•ë‚¨êµ¬',
    'êµëŒ€ì—­':'ì„œì´ˆêµ¬','ë‚¨ë¶€í„°ë¯¸ë„':'ì„œì´ˆêµ¬',
    # â”€â”€ ì˜ë“±í¬Â·ì‹ ì´Œê¶Œ
    'ì˜ë“±í¬ì—­':'ì˜ë“±í¬êµ¬','ë‹¹ì‚°ì—­':'ì˜ë“±í¬êµ¬',
    'ê³µë•ì—­':'ë§ˆí¬êµ¬','í™ëŒ€/í•©ì •':'ë§ˆí¬êµ¬','ë™êµ/ì—°ë‚¨':'ë§ˆí¬êµ¬','ë§ì›ì—­':'ë§ˆí¬êµ¬',
    'ì‹ ì´Œ/ì´ëŒ€':'ì„œëŒ€ë¬¸êµ¬',
    # â”€â”€ ê¸°íƒ€ê¶Œ
    'ê±´ëŒ€ì…êµ¬':'ê´‘ì§„êµ¬','ê²½í¬ëŒ€':'ë™ëŒ€ë¬¸êµ¬','ì²­ëŸ‰ë¦¬':'ë™ëŒ€ë¬¸êµ¬','ì¥ì•ˆë™':'ë™ëŒ€ë¬¸êµ¬',
    'ìƒë´‰ì—­':'ì¤‘ë‘êµ¬','ìƒê³„ì—­':'ë…¸ì›êµ¬',
    'ê¹Œì¹˜ì‚°ì—­':'ê°•ì„œêµ¬','í™”ê³¡':'ê°•ì„œêµ¬',
    'ëšì„¬':'ì„±ë™êµ¬','ì™•ì‹­ë¦¬':'ì„±ë™êµ¬',
    'ì²œí˜¸':'ê°•ë™êµ¬','ì ì‹¤/ì†¡íŒŒ':'ì†¡íŒŒêµ¬','ì ì‹¤ìƒˆë‚´ì—­':'ì†¡íŒŒêµ¬',
    'ëª©ë™':'ì–‘ì²œêµ¬','ì–‘ì¬ì—­':'ì„œì´ˆêµ¬',
    'ë…¸ëŸ‰ì§„':'ë™ì‘êµ¬','ì‚¬ë‹¹':'ë™ì‘êµ¬',
    'ì´íƒœì›':'ìš©ì‚°êµ¬','ìš©ì‚°ì—­':'ìš©ì‚°êµ¬','ìˆ™ëª…ì—¬ëŒ€':'ìš©ì‚°êµ¬',
    'ë…ì‚°/ì‹œí¥':'ê¸ˆì²œêµ¬','ì—°ì‹ ë‚´':'ì€í‰êµ¬','ë¶ˆê´‘ì—­':'ì€í‰êµ¬',
    'ì„±ì‹ ì—¬ëŒ€':'ì„±ë¶êµ¬','ë¯¸ì•„ì‚¬ê±°ë¦¬':'ê°•ë¶êµ¬','ìˆ˜ìœ ':'ê°•ë¶êµ¬',
    'ì˜¤ë¥˜ë™ì—­':'êµ¬ë¡œêµ¬','ì‹ ë¦¼ì—­':'ê´€ì•…êµ¬','ì„œìš¸ëŒ€ì…êµ¬ì—­':'ê´€ì•…êµ¬',
}

# ===============================================================
# 6)  ì˜ˆì¸¡ ë£¨í”„
# ===============================================================

results, tv_scores = [], []

for gu in w['gu_name'].unique():
    gdf = (w[w['gu_name'] == gu]
             .set_index('date')
             .sort_index())

    for cat in TARGET_CATS:
        s = gdf[cat]
        if s.sum() == 0: continue

        # 6ê°œ ì–¸íŒ¨í‚¹
        pred, true, r2_tr, r2_val, wm_tr, wm_val = xgb_forecast(
            s, test_days=TEST_DAYS, plot_curve=False
        )

        dates = s.index[-TEST_DAYS:]
        results.extend([[gu, cat, d, a, p]
                        for d, a, p in zip(dates, true, pred)])
        tv_scores.append([gu, cat, r2_tr, r2_val, wm_tr, wm_val])


df = pd.DataFrame(results, columns=['gu','cat','date','actual','pred'])\
         .reset_index(drop=True)      # ì¸ë±ìŠ¤ â†’ RangeIndex
df.index.name = None                  # ì¸ë±ìŠ¤ì— êµ¬(gu) ê°™ì€ ì´ë¦„ì´ ë‚¨ì•„ ìˆì§€ ì•Šê²Œ


# â”€â”€ ì €ë¬¼ëŸ‰(í•˜ìœ„ 30 %Â·800ê±´ ë¯¸ë§Œ) ì œì™¸

def drop_low(df, pct=0.00, abs_cut=0):
    df = df.reset_index(drop=True)           # â˜… ì¸ë±ìŠ¤ â†’ ì»¬ëŸ¼ ê°•ì œ ë³€í™˜
    def _mask(g):
        th = max(g.actual.quantile(pct), abs_cut)
        return g[g.actual >= th]
    return df.groupby(['gu', 'cat'], group_keys=False).apply(_mask)



df_filt = drop_low(df)
         # ì €ë¬¼ëŸ‰ í–‰ ì œê±°
df_filt = df_filt.reset_index(drop=True)   # â˜… INDEX ì •ë¦¬


df_tv = pd.DataFrame(
    tv_scores,
    columns=[
      'gu','cat',
      'R2_train','R2_val',
      'wMAPE_train','wMAPE_val'
    ]
)
print("\n=== í•™ìŠµ/ê²€ì¦ RÂ² Â· wMAPE ===")
display(df_tv.sort_values('wMAPE_val'))

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# í‰ê·  ì§€í‘œ ê³„ì‚°Â·ì¶œë ¥
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
avg_r2_tr    = df_tv['R2_train'].mean()
avg_r2_val   = df_tv['R2_val'].mean()
avg_wmape_tr = df_tv['wMAPE_train'].mean()
avg_wmape_val= df_tv['wMAPE_val'].mean()

print(f"\n=== ì „ì²´ í‰ê·  ì§€í‘œ ===")
print(f"RÂ²  (train) : {avg_r2_tr:.3f}")
print(f"RÂ²  (val)   : {avg_r2_val:.3f}")
print(f"wMAPE (train): {avg_wmape_tr:5.2f}%")
print(f"wMAPE (val)  : {avg_wmape_val:5.2f}%")

from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import make_scorer
import xgboost as xgb
import numpy as np

# 1) wMAPE ìŠ¤ì½”ì–´ëŸ¬ ì •ì˜ (ê¸°ì¡´ê³¼ ë™ì¼)
def wmape_sklearn(y_true, y_pred):
    y_true = np.expm1(y_true)
    y_pred = np.expm1(y_pred)
    return np.sum(np.abs(y_true - y_pred)) / np.sum(np.abs(y_true))

wmape_scorer = make_scorer(wmape_sklearn, greater_is_better=False)

# 2) í”¼ì²˜Â·ë¼ë²¨ ì¤€ë¹„ (ê¸°ì¡´ê³¼ ë™ì¼)
df_feat = make_feature_df(series)
X = df_feat.drop(columns='y').values
y = df_feat['y'].values

# 3) ì‹œê³„ì—´ 4-fold
tscv = TimeSeriesSplit(n_splits=4, test_size=TEST_DAYS)

# 4) ë¶„í¬(ê·¸ë¦¬ë“œ) ì •ì˜
param_dist = {
    'max_depth':        [5, 7, 9, 11],
    'learning_rate':    [0.005, 0.01, 0.02, 0.05],
    'reg_alpha':        [0.0, 0.1, 0.5],
    'reg_lambda':       [0.0, 0.5, 1.0],
    'subsample':        [0.7, 0.8, 0.9],
    'colsample_bytree': [0.7, 0.8, 0.9]
}

# 5) RandomizedSearchCV ì„¸íŒ…
rscv = RandomizedSearchCV(
    estimator = xgb.XGBRegressor(
        objective='reg:squarederror',
        tree_method='hist',
        n_estimators=150,
        seed=RAND_SEED,
        verbosity=0
    ),
    param_distributions = param_dist,
    n_iter   = 50,           # ì‹œë„í•  ì¡°í•© ìˆ˜ (ì˜ˆ: 50)
    scoring  = wmape_scorer,
    cv       = tscv,
    random_state = RAND_SEED,
    verbose  = 2,
    n_jobs   = -1
)

# 6) íƒìƒ‰ ì‹¤í–‰
rscv.fit(X, y)

# 7) ê²°ê³¼ í™•ì¸
best = rscv.best_params_
print("â–¶ Best Params   :", best)
print("â–¶ Best wMAPE avg:", -rscv.best_score_ * 100, "%")

# í•œê¸€ í°íŠ¸ ì„¤ì¹˜
!apt-get -qq update
!apt-get -qq install -y fonts-nanum

import logging, warnings, matplotlib.pyplot as plt

# 1) findfont ë¡œê·¸ ë ˆë²¨ ERRORë¡œ ë‚®ì¶”ê¸°
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# 2) í•œê¸€ ê¸€ë¦¬í”„ ëˆ„ë½ ê²½ê³  ë¬´ì‹œ
warnings.filterwarnings(
    "ignore",
    message=".*missing from font.*",
    module="matplotlib.font_manager"
)

# 3) ë‚˜ëˆ”ê³ ë”• í°íŠ¸ ì ìš© (Colab ì „ìš©) ë° ë§ˆì´ë„ˆìŠ¤ ê¸°í˜¸ ê¹¨ì§ ë°©ì§€
plt.rcParams['font.family']        = 'NanumGothic'
plt.rcParams['axes.unicode_minus'] = False

# --------------------------------------------------
# 9) PREDICT NEXT MONTH & SCORE (XGBoost - feature_names ê°•ì œ ë™ê¸°í™”)
# --------------------------------------------------
print('[7] Predict & Score')

# â‘  ìµœì‹  ì›”(ê°€ì¥ ìµœê·¼ ê´€ì¸¡ì¹˜)ë§Œ ì¶”ì¶œ â†’ ì˜ˆì¸¡ ëŒ€ìƒ ë°ì´í„°
latest = panel.groupby('gu_name').tail(1)

# â‘¡ ì‹¤ì œ í•™ìŠµì— ì‚¬ìš©ëœ feature ì´ë¦„ ì¶”ì¶œ
if best_model is not None:
    model_feat_names = list(best_model.estimators_[0].feature_names_in_)
else:
    model_feat_names = list(next(iter(model_dict.values())).feature_names_in_)

# â‘¢ panelì— ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” featureê°€ ì—†ìœ¼ë©´ NaN(ë˜ëŠ” 0)ìœ¼ë¡œ ê°•ì œ ì¶”ê°€
panel_fixed = panel.copy()
for f in model_feat_names:
    if f not in panel_fixed.columns:
        panel_fixed[f] = 0  # ë˜ëŠ” np.nanë„ ê°€ëŠ¥ (ì˜ˆì¸¡ì—” 0ì´ ë” ì•ˆì „)

# â‘£ panelì˜ feature ì»¬ëŸ¼ ìˆœì„œë¥¼ ëª¨ë¸ ìˆœì„œì— ë§ê²Œ ì •ë ¬
X_df = panel_fixed[model_feat_names]
X_future = X_df.loc[latest.index]

# â‘¤ ì˜ˆì¸¡
if best_model is not None:
    yhat = best_model.predict(X_future)
else:
    yhat = np.column_stack([
        model_dict[c].predict(X_future)
        for c in TARGET_CATS
    ])

# â‘¥ ì˜ˆì¸¡ ê²°ê³¼ DataFrameí™”
n_targets = yhat.shape[1] if len(yhat.shape) > 1 else 1
target_cats_used = TARGET_CATS[:n_targets]
ypred = pd.DataFrame(
    yhat,
    columns=[f'pred_{c}' for c in target_cats_used],
    index=latest.index
)

# â‘¦ ìì¹˜êµ¬ ì´ë¦„ê³¼ ì˜ˆì¸¡ê°’ í•©ì¹˜ê¸°
res = latest[['gu_name']].join(ypred)

# â‘§ ìˆ˜ìš” ì§€í‘œ(demand_idx) ê³„ì‚°
res['demand_idx'] = res.filter(like='pred_').max(axis=1)
res['demand_idx'] /= res['demand_idx'].max() if res['demand_idx'].max() != 0 else 1

# â‘¨ ì¶”ì²œ ì¹´í…Œê³ ë¦¬(max_cat)
res['max_cat'] = (
    res.filter(like='pred_')
       .idxmax(axis=1)
       .str.replace('pred_', '')
)

# â‘© staticê³¼ ë³‘í•©
res = res.merge(static, on='gu_name', how='left')

# â‘ª vacancy_idx ê³„ì‚°
res['vacancy_idx'] = 1 - res['vacancy_pct'] / res['vacancy_pct'].max() if res['vacancy_pct'].max() != 0 else 1

# â‘« ìµœì¢… score ê³„ì‚°
res['score'] = (
    res['demand_idx'] * 0.6 +
    res['vacancy_idx'] * 0.3 +
    res['access_idx']  * 0.1
)

# â‘¬ ìì¹˜êµ¬ë³„ ì˜ˆì¸¡ê°’ ì¶œë ¥
ypred_with_gu = latest[['gu_name']].join(ypred)
print(ypred_with_gu.to_string())

# =============================================================================
# ì‹œê°í™” ë¸”ë¡ (XGB MultiOutputRegressor ë²„ì „, ì¬í•™ìŠµ ì—†ì´ ì‹¤í–‰)
# =============================================================================
import warnings, logging, matplotlib.pyplot as plt
import matplotlib.font_manager as fm
import seaborn as sns
import numpy as np, pandas as pd
from xgboost import XGBRegressor

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ í™˜ê²½ ì„¤ì • (ë™ì¼) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
font_path = '/content/NanumGothic.ttf'
fm.fontManager.addfont(font_path)
plt.rcParams['font.family']        = fm.FontProperties(fname=font_path).get_name()
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings("ignore", category=UserWarning,
                        message=".*missing from font\\(s\\).*")
warnings.filterwarnings("ignore", category=FutureWarning,
                        message=".*Passing `palette` without assigning `hue`.*")
logging.getLogger('matplotlib.font_manager').setLevel(logging.ERROR)

# =============================================================================
# 1) ê²€ì¦ ì„¸íŠ¸ ì˜ˆì¸¡ê°’ (y_pred_final) ê³„ì‚°
# =============================================================================
y_pred_final = pd.DataFrame(
    best_model.predict(Xte),
    columns=Yte.columns,
    index=Yte.index
)

# =============================================================================
# 2) PCA ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ ì‹œê°í™”
#    â€” pca.explained_variance_ratio_ ì‚¬ìš©
# =============================================================================
explained = pca.explained_variance_ratio_ * 100

plt.figure(figsize=(8, 4))
sns.barplot(
    x=['PC1', 'PC2', 'PC3'],
    y=explained[:3],
    color='skyblue'
)
for idx, val in enumerate(explained[:3]):
    plt.text(
        idx,
        val + 1,
        f'{val:.1f}%',
        ha='center',
        va='bottom',
        fontsize=10
    )
plt.title('PCA ì£¼ì„±ë¶„ë³„ ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ (%)', fontsize=14)
plt.xlabel('ì£¼ì„±ë¶„', fontsize=12)
plt.ylabel('ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨ (%)', fontsize=12)
plt.ylim(0, 100)
plt.tight_layout()
plt.show()


# =============================================================================
# 3) ìì¹˜êµ¬ë³„ ì •ê·œí™” ì§€í‘œ ë¹„êµ
#    â€” res DataFrameì— ['pop_idx', 'access_idx', 'vacancy_idx'] ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •
# =============================================================================
norm_vars = ['pop_idx', 'access_idx', 'vacancy_idx']
existing = [v for v in norm_vars if v in res.columns]

if existing:
    melted = res[['gu_name'] + existing].melt(
        id_vars='gu_name',
        var_name='ì§€í‘œ',
        value_name='ê°’'
    )

    plt.figure(figsize=(12, 6))
    sns.barplot(
        data=melted,
        x='gu_name',
        y='ê°’',
        hue='ì§€í‘œ',
        palette='Set2'
    )
    plt.title('ìì¹˜êµ¬ë³„ ì •ê·œí™” ì§€í‘œ ë¹„êµ', fontsize=14)
    plt.xlabel('ìì¹˜êµ¬', fontsize=12)
    plt.ylabel('ì •ê·œí™” ê°’', fontsize=12)
    plt.xticks(rotation=45)
    plt.legend(title='ì§€í‘œ', loc='best', frameon=True, fontsize=10, title_fontsize=11)
    plt.tight_layout()
    plt.show()


# =============================================================================
# 4) ìì¹˜êµ¬ë³„ ì˜ˆì¸¡ ë¬¼ë™ëŸ‰ ì´í•© (pred_total) ì‹œê°í™”
#    â€” res DataFrameì— pred_â€¦ ì»¬ëŸ¼ì´ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
# =============================================================================
pred_cols = [c for c in res.columns if isinstance(c, str) and c.startswith('pred_')]
res['pred_total'] = res[pred_cols].sum(axis=1)

plt.figure(figsize=(12, 6))
sns.barplot(
    data=res.sort_values('pred_total', ascending=False),
    x='gu_name',
    y='pred_total',
    color='coral'
)
plt.title('ìì¹˜êµ¬ë³„ ì˜ˆì¸¡ ë¬¼ë™ëŸ‰ ì´í•©', fontsize=14)
plt.xlabel('ìì¹˜êµ¬', fontsize=12)
plt.ylabel('ì˜ˆì¸¡ ì´í•©', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# =============================================================================
# 5) ìì¹˜êµ¬ë³„ ìµœì¢… ì¢…í•© ì ìˆ˜ (score) ì‹œê°í™”
#    â€” res DataFrameì— score ì»¬ëŸ¼ì´ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆë‹¤ê³  ê°€ì •
# =============================================================================
plt.figure(figsize=(12, 6))
sns.barplot(
    data=res.sort_values('score', ascending=False),
    x='gu_name',
    y='score',
    color='steelblue'
)
plt.title('ìì¹˜êµ¬ë³„ ìµœì¢… ì¢…í•© ì ìˆ˜', fontsize=14)
plt.xlabel('ìì¹˜êµ¬', fontsize=12)
plt.ylabel('ì ìˆ˜', fontsize=12)
plt.xticks(rotation=45)
plt.tight_layout()
plt.show()


# =============================================================================
# 6) XGBoost Feature Importances  (RandomForest â†’ XGBRegressor ë¡œ ë³€ê²½)
# =============================================================================
# MultiOutputRegressor ë‚´ë¶€ ê° XGBRegressor ì—ì„œ gain ê¸°ë°˜ ì¤‘ìš”ë„ë¥¼ ì§‘ê³„
fi_dicts = []
feature_names = Xtr.columns.tolist()

for est in best_model.estimators_:                     # ê° íƒ€ê¹ƒë³„ XGBRegressor
    booster = est.get_booster()
    # gain ê¸°ë°˜ ì¤‘ìš”ë„ dict {f0:â€¦, f1:â€¦}
    scores = booster.get_score(importance_type='gain')
    # get_score ëŠ” feature ìˆœì„œë¥¼ f0,f1,â€¦ ë¡œ ë°˜í™˜ â†’ ì¸ë±ìŠ¤ ë§µ í•„ìš”
    imp = np.zeros(len(feature_names))
    for f, val in scores.items():
        if f in feature_names:
            idx = feature_names.index(f)
            imp[idx] = val
    fi_dicts.append(imp)


mean_importances = np.mean(fi_dicts, axis=0)
df_fi = (pd.DataFrame({'feature': feature_names,
                       'importance': mean_importances})
           .sort_values('importance', ascending=False))

top_n = 10
plt.figure(figsize=(8, 6))
sns.barplot(
    data=df_fi.head(top_n),
    x='importance',
    y='feature',
    palette='viridis'
)
plt.title('XGBoost Feature Importances (ìƒìœ„ 10ê°œ)', fontsize=14)
plt.xlabel('ì¤‘ìš”ë„ (Gain ê¸°ë°˜)', fontsize=12)
plt.ylabel('í”¼ì²˜', fontsize=12)
plt.tight_layout()
plt.show()


# =============================================================================
# 7) ê²€ì¦ ì„¸íŠ¸ ì‹¤ì œ vs ì˜ˆì¸¡ ì‚°ì ë„
#    â€” Xte, Yte, y_pred_final ì‚¬ìš©
# =============================================================================
categories = Yte.columns.tolist()
plt.figure(figsize=(16, 4 * len(categories)))

for i, cat in enumerate(categories, start=1):
    plt.subplot(len(categories), 1, i)
    plt.scatter(
        Yte[cat],
        y_pred_final[cat],
        alpha=0.6,
        color='teal',
        edgecolor='k',
        s=40
    )
    max_val = max(Yte[cat].max(), y_pred_final[cat].max())
    plt.plot([0, max_val], [0, max_val], 'r--', linewidth=1)
    plt.title(f'{cat} ì‹¤ì œ vs ì˜ˆì¸¡ (ê²€ì¦ ì„¸íŠ¸)', fontsize=12)
    plt.xlabel('ì‹¤ì œ ê°’', fontsize=10)
    plt.ylabel('ì˜ˆì¸¡ ê°’', fontsize=10)
    plt.grid(alpha=0.3)

plt.tight_layout()
plt.show()


# =============================================================================
# 8) ê²€ì¦ ì„¸íŠ¸ ì”ì°¨ ë¶„í¬ (Residual Distribution, KDE)
#    â€” y_pred_final, Yte ì‚¬ìš©
# =============================================================================
plt.figure(figsize=(8, 6))
for cat in categories:
    residuals = (y_pred_final[cat] - Yte[cat]).dropna()
    sns.kdeplot(
        residuals,
        fill=True,
        alpha=0.4,
        label=cat
    )

plt.title('ê²€ì¦ ì„¸íŠ¸ ì”ì°¨ ë¶„í¬ (Residual Distribution)', fontsize=14)
plt.xlabel('ì”ì°¨ (ì˜ˆì¸¡ - ì‹¤ì œ)', fontsize=12)
plt.ylabel('ë°€ë„', fontsize=12)
plt.legend(title='ì¹´í…Œê³ ë¦¬', fontsize=10, title_fontsize=11)
plt.tight_layout()
plt.show()

# ------------------------------------------------------------
# 11) CATEGORY-SPECIFIC MAPS  âš‘ auto-detect gu_name column
# ------------------------------------------------------------
import geopandas as gpd, pandas as pd, folium, numpy as np
from branca.colormap import linear
from folium.features   import GeoJson, GeoJsonTooltip
from pathlib import Path, PurePath
import re, warnings

def _normalize_gu(s: pd.Series) -> pd.Series:
    """'ì„œìš¸íŠ¹ë³„ì‹œ ì¢…ë¡œêµ¬' â†’ 'ì¢…ë¡œêµ¬' ë¡œ, ê³µë°±Â·ì ‘ë‘ì‚¬ ì œê±°"""
    return (
        s.astype(str)
         .str.replace(r'\s+', ' ', regex=True).str.strip()
         .str.replace(r'^ì„œìš¸íŠ¹ë³„ì‹œ\s*', '', regex=True)
         .str.replace(r'^ì„œìš¸\s*',       '', regex=True)
         .str.replace(r'(íŠ¹ë³„|ê´‘ì—­)?ì‹œ$', '', regex=True)   # â€¦ì‹œ ì ‘ë¯¸ì‚¬ ì œê±°
    )

def _find_gu_column(gdf: gpd.GeoDataFrame) -> str|None:
    """25ê°œ ì´ìƒ â€˜â€¦êµ¬â€™(ë˜ëŠ” êµ°Â·ì‹œ) ë¡œ ëë‚˜ëŠ” ê°’ì´ ìˆëŠ” ì²« ë²ˆì§¸ ì»¬ëŸ¼ ë°˜í™˜"""
    for c in gdf.columns:
        if gdf[c].dtype == object:
            vals = _normalize_gu(gdf[c])
            uniq = vals.unique()
            # â€˜â€¦êµ¬â€™ â€§ â€˜â€¦êµ°â€™ â€§ â€˜â€¦ì‹œâ€™ ë¡œ ëë‚˜ëŠ” ê°’ ê°œìˆ˜
            gu_like = [v for v in uniq if re.search(r'(êµ¬|êµ°|ì‹œ)$', str(v))]
            if len(gu_like) >= 25:
                return c
    return None

def make_maps(
        res: pd.DataFrame,             # ì˜ˆì¸¡ ê²°ê³¼ (25Ã—N)
        shp_path: str|Path,            # ìì¹˜êµ¬ ê²½ê³„ shp
        out_dir : str|Path             # HTML ì €ì¥ í´ë”
    ) -> None:

    out_dir = Path(out_dir); out_dir.mkdir(exist_ok=True, parents=True)

    # 1) ê²½ê³„ ë¡œë“œ & WGS84
    g = (gpd.read_file(shp_path, encoding='cp949', errors='ignore')
            .to_crs(epsg=4326))

    # 2) gu_name ì»¬ëŸ¼ ì°¾ê¸°
    col = _find_gu_co_

def clean_gu(s: pd.Series) -> pd.Series:
    """
    1) ì „ê° ê³µë°±( \u3000 ) ì œê±°
    2) íƒ­Â·ê°œí–‰ Â·ì¼ë°˜ ê³µë°± strip
    3) 'ì„œìš¸íŠ¹ë³„ì‹œ ' ê°™ì€ ì ‘ë‘ì–´ ì œê±°(ìˆë‹¤ë©´)
    """
    return (s.astype(str)
              .str.replace('\u3000', '', regex=False)      # ì „ê° ê³µë°±
              .str.strip()                                 # ì–‘ìª½ ê³µë°±Â·íƒ­
              .str.replace(r'^ì„œìš¸íŠ¹ë³„ì‹œ\s*', '', regex=True))  # ì ‘ë‘ì–´

def load_gu_polygons_fixed(shp_path: str) -> gpd.GeoDataFrame:
    g = (gpd.read_file(shp_path, encoding='cp949')
           .to_crs(epsg=4326))

    # LABEL â†’ 'ì§€ì—­ìƒí™œê¶Œ(ê°•ë‚¨êµ¬_ëŒ€ì¹˜ë„ê³¡)' â”€â”€> 'ê°•ë‚¨êµ¬'
    g['gu_name'] = (
        g['LABEL']
        .str.extract(r'_(.+?)\)$', expand=False)   # 'ê°•ë‚¨êµ¬'
        .pipe(clean_gu)
    )

    # NaN(ì¶”ì¶œ ì‹¤íŒ¨) í–‰ ì œê±°
    g = g.dropna(subset=['gu_name'])

    # â˜… ì¤‘ë³µë˜ëŠ” gu_nameì´ ì—¬ì „íˆ ìˆëŠ”ì§€ í™•ì¸
    dup_check = g['gu_name'].value_counts().head()
    print('ğŸ”  ì¶”ì¶œ í›„ gu_name ë¹ˆë„ Top5\n', dup_check, '\n')

    # dissolve
    g25 = (g[['gu_name', 'geometry']]
           .dissolve(by='gu_name', as_index=False))

    assert len(g25) == 25, f'âŒ ì—¬ì „íˆ 25ê°œê°€ ì•„ë‹˜: {len(g25)}'
    return g25

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# CATEGORY-SPECIFIC MAPS  â€•  í•œ ë²ˆì— ì‹¤í–‰ ê°€ëŠ¥í•œ ì™„ì„± ë¸”ë¡
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import re
from pathlib import Path

import geopandas as gpd
import pandas as pd
import folium
from branca.colormap import linear

# --------------------------------------------------------------
# 1) SHP â†’ 25ê°œ â€˜êµ¬â€™ í´ë¦¬ê³¤ìœ¼ë¡œ ì •ë¦¬
# --------------------------------------------------------------
def load_seoul_gu(shp_path: str) -> gpd.GeoDataFrame:
    # â‘  â€˜ì›ë³¸ CRSâ€™ ëª…ì‹œ â€• KGD2002 Central Belt (ESRI:102082 â‰ˆ EPSG:5174)
    g = (gpd.read_file(shp_path, encoding="cp949")
           .set_crs("EPSG:5174", allow_override=True)   # â˜… ì¤‘ìš”
           .to_crs(epsg=4326))

    # LABEL ì† ê´„í˜¸ ì•ˆ ì²« ë¸”ë¡(â€˜ê°•ë‚¨êµ¬_â€¦â€™) â†’ 'ê°•ë‚¨êµ¬'
    def _pick_gu(lbl):
        m = re.search(r"\(([^)]+)\)", str(lbl))
        if m:
            part = m.group(1).split("_")[0]
            return part if part.endswith("êµ¬") else None
        return None

    g["gu_name"] = g["LABEL"].apply(_pick_gu)
    g = g.dropna(subset=["gu_name"])

    # ê°™ì€ gu_name ë¼ë¦¬ dissolve í•´ì„œ 25ê°œ ë©´ í™•ë³´
    g25 = (g[["gu_name", "geometry"]]
           .dissolve(by="gu_name", as_index=False))

    assert len(g25) == 25, f"âŒ êµ¬ ê°œìˆ˜ê°€ {len(g25)} (â‰ 25)"
    return g25


# --------------------------------------------------------------
# 2) ì¹´í…Œê³ ë¦¬ë³„ Choropleth HTML ì €ì¥
# --------------------------------------------------------------
def save_category_maps(res_df: pd.DataFrame,
                       shp_path: str,
                       out_dir: str,
                       target_cats: list[str]) -> None:

    out_dir = Path(out_dir)
    out_dir.mkdir(exist_ok=True, parents=True)

    # (1) ê²½ê³„ + ì˜ˆì¸¡ê°’ ë³‘í•©
    shp_gu  = load_seoul_gu(shp_path)
    res_g   = shp_gu.merge(res_df.reset_index(), on="gu_name", how="left")

    # (2) ì¹´í…Œê³ ë¦¬ë³„ ì§€ë„ ìƒì„±
    for cat in target_cats:
        col   = f"pred_{cat}"
        vals  = res_g[col].dropna()
        vmin, vmax = (0, 1) if vals.empty else (vals.min(), vals.max())
        if vmin == vmax:            # ê°’ì´ í•œ ì ë¿ì¼ ë•Œ
            vmax = vmin + 1e-6

        m = folium.Map(location=[37.56, 126.97],
                       zoom_start=11,
                       tiles="CartoDB positron")

        # 2-1) ìƒ‰ìƒ ë ˆì´ì–´
        folium.Choropleth(
            geo_data   = res_g.to_json(),
            data       = res_g,
            columns    = ["gu_name", col],
            key_on     = "feature.properties.gu_name",
            fill_color = "YlOrRd",
            fill_opacity = 0.8,
            line_opacity = 0.3,
            nan_fill_color = "#ffffff",
            legend_name   = f"{cat} demand"
        ).add_to(m)

        # 2-2) íˆ´íŒ(êµ¬ ì´ë¦„ & ì˜ˆì¸¡ê°’)
        folium.GeoJson(
            res_g,
            name="tooltip-layer",
            style_function = lambda _ : {"color":"transparent", "fillOpacity":0},
            tooltip = folium.features.GeoJsonTooltip(
                fields   = ["gu_name", col],
                aliases  = ["êµ¬", "ì˜ˆì¸¡"],
                localize = True,
                sticky   = False
            )
        ).add_to(m)

        # (3) ì €ì¥
        out_path = out_dir / f"map_{cat}.html"
        m.save(out_path)
        print(f"âœ”  saved  â†’  {out_path}")


# --------------------------------------------------------------
# 3) ì‹¤í–‰ ì˜ˆì‹œ  (ì´ë¯¸ res, TARGET_CATS, OUTPUT_DIR ì¡´ì¬)
# --------------------------------------------------------------
# 1. columns str ë³€í™˜
if isinstance(res.columns, pd.MultiIndex):
    res.columns = ['_'.join(map(str, col)).strip('_') for col in res.columns]
else:
    res.columns = [str(col) for col in res.columns]

# 2. index str ë³€í™˜
if isinstance(res.index, pd.MultiIndex):
    res.index = ['_'.join(map(str, idx)).strip('_') for idx in res.index]
else:
    res.index = [str(idx) for idx in res.index]

# 3. ëª¨ë“  ì…€ ë‚´ë¶€ì— tuple/dictê°€ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸ í›„ strë¡œ ë³€í™˜
for col in res.columns:
    res[col] = res[col].apply(lambda x: str(x) if isinstance(x, (tuple, dict)) else x)

# 4. save_category_maps ì‹¤í–‰
save_category_maps(
    res_df   = res,
    shp_path = "/content/seoul_gu.shp",
    out_dir  = OUTPUT_DIR,
    target_cats = TARGET_CATS
)




# ë‹¤ì‹œ GeoDataFrameìœ¼ë¡œ ì½ì–´ì„œ ì‚¬ìš©!
g = gpd.read_file("/content/seoul_gu.shp", encoding="cp949")
g_ll = g.to_crs(epsg=4326)
print(g_ll.total_bounds)